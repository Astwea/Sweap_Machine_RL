#!/usr/bin/env python3
"""
TensorBoard ç›‘æ§é…ç½®å’Œå·¥å…·
ç”¨äº MyDog å¼ºåŒ–å­¦ä¹ è®­ç»ƒè¿‡ç¨‹çš„å…¨é¢ç›‘æ§
"""

import os
import time
import torch
from torch.utils.tensorboard import SummaryWriter
import numpy as np
from typing import Dict, Any, Optional

class TensorBoardMonitor:
    """å¢å¼ºç‰ˆ TensorBoard ç›‘æ§å™¨"""
    
    def __init__(self, log_dir: str, experiment_name: str = "mydog_rl"):
        """
        åˆå§‹åŒ– TensorBoard ç›‘æ§å™¨
        
        Args:
            log_dir: æ—¥å¿—ç›®å½•
            experiment_name: å®éªŒåç§°
        """
        self.log_dir = log_dir
        self.experiment_name = experiment_name
        self.writer = SummaryWriter(log_dir=os.path.join(log_dir, experiment_name))
        
        # ç›‘æ§æŒ‡æ ‡åˆ†ç±»
        self.metrics_categories = {
            "Reward": [
                "progress_reward", "lateral_penalty", "direction_reward", 
                "goal_bias", "action_rate_penalty", "action_mag_penalty", 
                "imitation_reward", "Total"
            ],
            "Environment": [
                "Distance_to_Target", "Lateral_Error", "Heading_Error",
                "Robot_Speed", "Action_Magnitude", "Action_Rate"
            ],
            "Performance": [
                "Episode_Length", "Total_Distance", "Average_Speed",
                "Max_Lateral_Error", "Step_Time", "FPS"
            ],
            "Episode": [
                "Episode_Reward", "Episode_Length", "Success_Rate",
                "Total_Distance", "Final_Distance", "Max_Lateral_Error"
            ],
            "Training": [
                "Best_Reward", "Average_Reward", "Total_Episodes",
                "Elapsed_Time", "Steps_Per_Second"
            ]
        }
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            "episode_count": 0,
            "total_steps": 0,
            "best_reward": float('-inf'),
            "start_time": time.time()
        }
    
    def log_reward_metrics(self, rewards: Dict[str, torch.Tensor], step: int):
        """è®°å½•å¥–åŠ±æŒ‡æ ‡"""
        for key, value in rewards.items():
            if isinstance(value, torch.Tensor):
                self.writer.add_scalar(f"Reward/{key}", value.mean().item(), step)
            else:
                self.writer.add_scalar(f"Reward/{key}", value, step)
    
    def log_environment_metrics(self, metrics: Dict[str, Any], step: int):
        """è®°å½•ç¯å¢ƒæŒ‡æ ‡"""
        for key, value in metrics.items():
            if isinstance(value, torch.Tensor):
                self.writer.add_scalar(f"Environment/{key}", value.mean().item(), step)
            else:
                self.writer.add_scalar(f"Environment/{key}", value, step)
    
    def log_performance_metrics(self, metrics: Dict[str, Any], step: int):
        """è®°å½•æ€§èƒ½æŒ‡æ ‡"""
        for key, value in metrics.items():
            self.writer.add_scalar(f"Performance/{key}", value, step)
    
    def log_episode_summary(self, episode_data: Dict[str, Any], episode_num: int):
        """è®°å½•å›åˆæ€»ç»“"""
        for key, value in episode_data.items():
            self.writer.add_scalar(f"Episode/{key}", value, episode_num)
        
        # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
        self.stats["episode_count"] = episode_num
        if "Episode_Reward" in episode_data:
            self.stats["best_reward"] = max(self.stats["best_reward"], episode_data["Episode_Reward"])
    
    def log_training_progress(self, step: int):
        """è®°å½•è®­ç»ƒè¿›åº¦"""
        elapsed_time = time.time() - self.stats["start_time"]
        steps_per_second = step / elapsed_time if elapsed_time > 0 else 0
        
        self.writer.add_scalar("Training/Elapsed_Time", elapsed_time, step)
        self.writer.add_scalar("Training/Steps_Per_Second", steps_per_second, step)
        self.writer.add_scalar("Training/Best_Reward", self.stats["best_reward"], step)
        self.writer.add_scalar("Training/Total_Episodes", self.stats["episode_count"], step)
    
    def log_hyperparameters(self, config: Dict[str, Any]):
        """è®°å½•è¶…å‚æ•°"""
        self.writer.add_hparams(
            hparam_dict=config,
            metric_dict={"best_reward": self.stats["best_reward"]}
        )
    
    def log_model_architecture(self, model):
        """è®°å½•æ¨¡å‹æ¶æ„"""
        # è¿™é‡Œå¯ä»¥æ·»åŠ æ¨¡å‹æ¶æ„çš„å¯è§†åŒ–
        pass
    
    def log_distributions(self, data: Dict[str, torch.Tensor], step: int):
        """è®°å½•æ•°æ®åˆ†å¸ƒ"""
        for key, values in data.items():
            if isinstance(values, torch.Tensor) and values.numel() > 0:
                self.writer.add_histogram(f"Distributions/{key}", values, step)
    
    def close(self):
        """å…³é—­ç›‘æ§å™¨"""
        self.writer.close()

def create_tensorboard_dashboard_config():
    """åˆ›å»º TensorBoard ä»ªè¡¨æ¿é…ç½®"""
    config = {
        "version": 1,
        "disable_theme": False,
        "timezone": "Asia/Shanghai",
        "scalars": {
            "layout": {
                "height": 300,
                "margin": 5
            },
            "smoothing": 0.6,
            "xAxis": "step",
            "yAxis": "value"
        },
        "histograms": {
            "layout": {
                "height": 300,
                "margin": 5
            },
            "smoothing": 0.6
        }
    }
    return config

def print_monitoring_guide():
    """æ‰“å°ç›‘æ§æŒ‡å—"""
    print("""
ğŸ¯ MyDog å¼ºåŒ–å­¦ä¹  TensorBoard ç›‘æ§æŒ‡å—
========================================

ğŸ“Š ä¸»è¦ç›‘æ§æŒ‡æ ‡åˆ†ç±»:

1. å¥–åŠ±æŒ‡æ ‡ (Reward/*)
   - progress_reward: è·¯å¾„æ¨è¿›å¥–åŠ±
   - lateral_penalty: ä¾§å‘è¯¯å·®æƒ©ç½š
   - direction_reward: æ–¹å‘å¥–åŠ±
   - goal_bias: ç›®æ ‡åå·®å¥–åŠ±
   - action_rate_penalty: åŠ¨ä½œå˜åŒ–ç‡æƒ©ç½š
   - action_mag_penalty: åŠ¨ä½œå¹…åº¦æƒ©ç½š
   - imitation_reward: æ¨¡ä»¿å­¦ä¹ å¥–åŠ±
   - Total: æ€»å¥–åŠ±

2. ç¯å¢ƒæŒ‡æ ‡ (Environment/*)
   - Distance_to_Target: åˆ°ç›®æ ‡è·ç¦»
   - Lateral_Error: ä¾§å‘è¯¯å·®
   - Heading_Error: èˆªå‘è¯¯å·®
   - Robot_Speed: æœºå™¨äººé€Ÿåº¦
   - Action_Magnitude: åŠ¨ä½œå¹…åº¦
   - Action_Rate: åŠ¨ä½œå˜åŒ–ç‡

3. æ€§èƒ½æŒ‡æ ‡ (Performance/*)
   - Episode_Length: å›åˆé•¿åº¦
   - Total_Distance: æ€»è¡Œé©¶è·ç¦»
   - Average_Speed: å¹³å‡é€Ÿåº¦
   - Max_Lateral_Error: æœ€å¤§ä¾§å‘è¯¯å·®
   - Step_Time: æ¯æ­¥æ‰§è¡Œæ—¶é—´
   - FPS: æ¯ç§’å¸§æ•°

4. å›åˆç»Ÿè®¡ (Episode/*)
   - Episode_Reward: å›åˆæ€»å¥–åŠ±
   - Episode_Length: å›åˆé•¿åº¦
   - Success_Rate: æˆåŠŸç‡
   - Total_Distance: æ€»è·ç¦»
   - Final_Distance: æœ€ç»ˆè·ç¦»
   - Max_Lateral_Error: æœ€å¤§ä¾§å‘è¯¯å·®

5. è®­ç»ƒç»Ÿè®¡ (Training/*)
   - Best_Reward: æœ€ä½³å¥–åŠ±
   - Average_Reward: å¹³å‡å¥–åŠ±
   - Total_Episodes: æ€»å›åˆæ•°
   - Elapsed_Time: å·²ç”¨æ—¶é—´
   - Steps_Per_Second: æ¯ç§’æ­¥æ•°

ğŸ”§ ä½¿ç”¨å»ºè®®:
- å…³æ³¨ Reward/Total çš„ä¸Šå‡è¶‹åŠ¿
- ç›‘æ§ Environment/Lateral_Error çš„ä¸‹é™
- è§‚å¯Ÿ Performance/FPS ä¿æŒç¨³å®š
- æ£€æŸ¥ Episode/Success_Rate çš„æå‡
- åˆ†æ Training/Best_Reward çš„å¢é•¿

ğŸ“ˆ ä¼˜åŒ–å»ºè®®:
- å¦‚æœå¥–åŠ±ä¸å¢é•¿ï¼Œæ£€æŸ¥å­¦ä¹ ç‡è®¾ç½®
- å¦‚æœä¾§å‘è¯¯å·®å¤§ï¼Œè°ƒæ•´å¥–åŠ±æƒé‡
- å¦‚æœFPSä½ï¼Œè€ƒè™‘å‡å°‘ç¯å¢ƒæ•°é‡
- å¦‚æœæˆåŠŸç‡ä½ï¼Œå¢åŠ è®­ç»ƒæ—¶é—´
""")

if __name__ == "__main__":
    print_monitoring_guide()
